{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u\n",
    "import glob\n",
    "import os\n",
    "from scipy.spatial import cKDTree\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, Manager, freeze_support\n",
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, classification_report"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T18:05:50.309022Z",
     "end_time": "2023-04-19T18:05:55.929891Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m master_catalog \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaster_updated - Copy.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, delimiter\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      2\u001B[0m master_catalog \u001B[38;5;241m=\u001B[39m master_catalog\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mID\u001B[39m\u001B[38;5;124m'\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      3\u001B[0m master_catalog \u001B[38;5;241m=\u001B[39m master_catalog\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDATATABLE\u001B[39m\u001B[38;5;124m'\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "master_catalog = pd.read_csv('master_updated - Copy.csv', delimiter=',')\n",
    "master_catalog = master_catalog.drop('ID', axis=1)\n",
    "master_catalog = master_catalog.drop('DATATABLE', axis=1)\n",
    "# master_catalog = master_catalog.drop('RADEG', axis=1)\n",
    "# master_catalog = master_catalog.drop('DECDEG', axis=1)\n",
    "master_catalog.dropna(inplace=True)\n",
    "# Create new columns for (i-g) and (g-i)\n",
    "# master_catalog['i-g'] = master_catalog['i'] - master_catalog['g']\n",
    "master_catalog['g-i'] = master_catalog['g'] - master_catalog['i']\n",
    "# master_catalog = master_catalog.drop(master_catalog==[2 ,3 , 5, 7, 8 ,9])\n",
    "# master_catalog_temp = master_catalog[master_catalog.CLASS==1]\n",
    "\n",
    "master_catalog = master_catalog[~master_catalog['CLASS'].isin([2 , 3, 5, 7, 8 ,9])]\n",
    "# class_dict = {1: 0, 4: 1, 6: 2, 2: 0, 3: 1, 9: 2, 8: 2, 7:2}\n",
    "class_dict = {1: 0, 4: 1, 6: 2}\n",
    "\n",
    "# relabel the 'CLASS' attribute using the map method\n",
    "master_catalog['CLASS'] = master_catalog['CLASS'].map(class_dict)\n",
    "print(master_catalog.head())\n",
    "print(len(master_catalog))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n",
      "1    1638\n",
      "0     735\n",
      "Name: CLASS, dtype: int64\n",
      "       RADEG     DECDEG       i       g    g-i\n",
      "0  10.603958  41.209639  20.696  22.014  1.318\n",
      "1  10.581171  40.872878  21.512  23.340  1.828\n",
      "2  23.924083  28.820972  22.424  22.940  0.516\n",
      "3  10.922904  41.407150  21.553  23.002  1.449\n",
      "4  10.144979  40.443692  22.805  23.006  0.201\n",
      "{0, 1}\n",
      "0    735\n",
      "1    735\n",
      "Name: CLASS, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = master_catalog.drop('CLASS', axis=1) # Features\n",
    "y = master_catalog['CLASS'] # Target variable\n",
    "print((set(y)))\n",
    "print(y.value_counts())\n",
    "class_counts = master_catalog['CLASS'].value_counts()\n",
    "\n",
    "# define the desired number of datapoints per class\n",
    "num_datapoints = min(class_counts)\n",
    "\n",
    "# sample the same number of datapoints from each class\n",
    "master_catalog_balanced = master_catalog.groupby('CLASS').apply(lambda x: x.sample(n=num_datapoints)).reset_index(drop=True)\n",
    "\n",
    "# print the modified dataframe\n",
    "# print(master_catalog_balanced.head())\n",
    "X = master_catalog_balanced.drop('CLASS', axis=1) # Features\n",
    "print(X.head())\n",
    "y = master_catalog_balanced['CLASS'] # Target variable\n",
    "print((set(y)))\n",
    "print(y.value_counts())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "## create DS9 regions for 'confirmed' GCs\n",
    "concat_thin_confirmed_gcs = pd.DataFrame(master_catalog_balanced)[master_catalog_balanced.CLASS==1]\n",
    "radius = '10.0\"'\n",
    "with open('ds9_regions/confirmed.reg', 'w') as f:\n",
    "    f.write('global color=green dashlist=8 3 width=2 font=\"helvetica 10 normal roman\" select=1 highlite=1 dash=0 '\n",
    "            'fixed=0 edit=1 move=1 delete=1 include=1 source=1 \\n')\n",
    "    f.write('fk5 \\n')\n",
    "    for n in range(0,len(concat_thin_confirmed_gcs)):\n",
    "        ra = str(concat_thin_confirmed_gcs.RADEG[n:n+1].values[0])\n",
    "        dec = str(concat_thin_confirmed_gcs.DECDEG[n:n+1].values[0])\n",
    "        f.write('circle('+ra+','+dec+','+radius+') \\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6530612244897959\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.7142857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74        76\n",
      "           1       0.74      0.63      0.68        71\n",
      "\n",
      "    accuracy                           0.71       147\n",
      "   macro avg       0.72      0.71      0.71       147\n",
      "weighted avg       0.72      0.71      0.71       147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train and test using Support Vector Machine (SVM)\n",
    "svm = SVC(kernel='rbf', random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm.predict(X_test_scaled)\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(classification_report(y_test, y_pred_svm))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8231292517006803\n",
      "Random Forest Precision: 0.8263686426951733\n",
      "Random Forest Recall: 0.8231292517006803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84        76\n",
      "           1       0.86      0.76      0.81        71\n",
      "\n",
      "    accuracy                           0.82       147\n",
      "   macro avg       0.83      0.82      0.82       147\n",
      "weighted avg       0.83      0.82      0.82       147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train and test using Random Forest Classifier\n",
    "rfc = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rfc.fit(X_train_scaled, y_train)\n",
    "y_pred_rfc = rfc.predict(X_test_scaled)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rfc))\n",
    "print(\"Random Forest Precision:\", precision_score(y_test, y_pred_rfc, average='weighted'))\n",
    "print(\"Random Forest Recall:\", recall_score(y_test, y_pred_rfc, average='weighted'))\n",
    "print(classification_report(y_test, y_pred_rfc))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# # Train and test using XGBoost\n",
    "# xgb = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "# xgb.fit(X_train_scaled, y_train)\n",
    "# y_pred_xgb = xgb.predict(X_test_scaled)\n",
    "# print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "# one_hot = pd.get_dummies(master_catalog['CLASS'], prefix='CLASS')\n",
    "#\n",
    "# # concatenate the one-hot vector with the original dataframe\n",
    "# master_catalog = pd.concat([master_catalog, one_hot], axis=1)\n",
    "#\n",
    "# # drop the original 'CLASS' attribute\n",
    "# master_catalog = master_catalog.drop('CLASS', axis=1)\n",
    "#\n",
    "# # print the modified dataframe\n",
    "# print(master_catalog)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(1470, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "values = np.array(master_catalog_balanced['CLASS'], dtype='int')\n",
    "n_values = 2\n",
    "print(n_values)\n",
    "onehot = np.eye(n_values)[values]\n",
    "print(onehot.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, onehot, test_size=0.1, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "331/331 [==============================] - 1s 964us/step - loss: 2.9191 - accuracy: 0.5737\n",
      "Epoch 2/20\n",
      "331/331 [==============================] - 0s 949us/step - loss: 0.7163 - accuracy: 0.6047\n",
      "Epoch 3/20\n",
      "331/331 [==============================] - 0s 867us/step - loss: 0.7026 - accuracy: 0.5949\n",
      "Epoch 4/20\n",
      "331/331 [==============================] - 0s 867us/step - loss: 0.6729 - accuracy: 0.6213\n",
      "Epoch 5/20\n",
      "331/331 [==============================] - 0s 864us/step - loss: 0.6650 - accuracy: 0.6017\n",
      "Epoch 6/20\n",
      "331/331 [==============================] - 0s 876us/step - loss: 0.6574 - accuracy: 0.6213\n",
      "Epoch 7/20\n",
      "331/331 [==============================] - 0s 843us/step - loss: 0.6606 - accuracy: 0.6115\n",
      "Epoch 8/20\n",
      "331/331 [==============================] - 0s 861us/step - loss: 0.6876 - accuracy: 0.6070\n",
      "Epoch 9/20\n",
      "331/331 [==============================] - 0s 867us/step - loss: 0.6663 - accuracy: 0.6145\n",
      "Epoch 10/20\n",
      "331/331 [==============================] - 0s 907us/step - loss: 0.6433 - accuracy: 0.6357\n",
      "Epoch 11/20\n",
      "331/331 [==============================] - 0s 867us/step - loss: 0.6523 - accuracy: 0.6243\n",
      "Epoch 12/20\n",
      "331/331 [==============================] - 0s 861us/step - loss: 0.6687 - accuracy: 0.6077\n",
      "Epoch 13/20\n",
      "331/331 [==============================] - 0s 873us/step - loss: 0.6498 - accuracy: 0.6334\n",
      "Epoch 14/20\n",
      "331/331 [==============================] - 0s 855us/step - loss: 0.6537 - accuracy: 0.6160\n",
      "Epoch 15/20\n",
      "331/331 [==============================] - 0s 907us/step - loss: 0.6568 - accuracy: 0.6304\n",
      "Epoch 16/20\n",
      "331/331 [==============================] - 0s 876us/step - loss: 0.6860 - accuracy: 0.5971\n",
      "Epoch 17/20\n",
      "331/331 [==============================] - 0s 1ms/step - loss: 0.6480 - accuracy: 0.6251\n",
      "Epoch 18/20\n",
      "331/331 [==============================] - 0s 1ms/step - loss: 0.6491 - accuracy: 0.6289\n",
      "Epoch 19/20\n",
      "331/331 [==============================] - 0s 970us/step - loss: 0.6631 - accuracy: 0.6115\n",
      "Epoch 20/20\n",
      "331/331 [==============================] - 0s 1ms/step - loss: 0.6716 - accuracy: 0.6175\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x25980ba1730>"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "# Define the neural network architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(15, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=4, verbose=1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step\n",
      "0    129\n",
      "1     18\n",
      "dtype: int64\n",
      "0    76\n",
      "1    71\n",
      "dtype: int64\n",
      "Neural Network Accuracy: 0.5578231292517006\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.92      0.68        76\n",
      "           1       0.67      0.17      0.27        71\n",
      "\n",
      "    accuracy                           0.56       147\n",
      "   macro avg       0.60      0.55      0.48       147\n",
      "weighted avg       0.60      0.56      0.48       147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# y_pred_nn = model.predict(X_test)\n",
    "# print(y_pred_nn)\n",
    "# y_pred_nn_classes = (y_pred_nn.argmax()).astype(int)\n",
    "pred = model.predict(X_test)\n",
    "prediction = np.argmax(pred, axis=-1)\n",
    "y_true = np.argmax(y_test, axis=-1)\n",
    "print((pd.DataFrame(prediction)).value_counts())\n",
    "print((pd.DataFrame(y_true)).value_counts())\n",
    "print(\"Neural Network Accuracy:\", accuracy_score(y_true, prediction))\n",
    "print(\"Classification Report: \", classification_report(y_true, prediction))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "# Create an instance of the Random Forest Classifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "rfc.fit(X_train, y_train)\n",
    "with open('rfc_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rfc, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "with open('rfc_model.pkl', 'rb') as f:\n",
    "    rfc = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7891156462585034\n"
     ]
    }
   ],
   "source": [
    "# Use the trained model to make predictions on the test data\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "#Importing essential libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Build SRF model\n",
    "SRF = RandomForestClassifier(n_estimators=150, random_state=0)\n",
    "#Create Stratified K-fold cross validation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "scoring = ('f1', 'recall', 'precision')\n",
    "#Evaluate SRF model\n",
    "# scores = cross_validate(SRF, X, y, scoring=scoring, cv=cv)\n",
    "# #Get average evaluation metrics\n",
    "# print('Mean f1: %.3f' % mean(scores['test_f1']))\n",
    "# print('Mean recall: %.3f' % mean(scores['test_recall']))\n",
    "# print('Mean precision: %.3f' % mean(scores['test_precision']))\n",
    "\n",
    "#Randomly spilt dataset to test and train set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "#Train SRF\n",
    "SRF.fit(X_train, y_train)\n",
    "#SRF prediction result\n",
    "y_pred = SRF.predict(X_test)\n",
    "#Create confusion matrix\n",
    "# fig = plot_confusion_matrix(SRF, X_test, y_test, display_labels=['Will Not Buy', 'Will Buy'], cmap='Greens')\n",
    "# plt.title('Standard Random Forest Confusion Matrix')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7653061224489796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.75      0.76       147\n",
      "           1       0.76      0.78      0.77       147\n",
      "\n",
      "    accuracy                           0.77       294\n",
      "   macro avg       0.77      0.77      0.77       294\n",
      "weighted avg       0.77      0.77      0.77       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TEST DATA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "with open('rfc_model.pkl', 'rb') as f:\n",
    "    rfc = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       RADEG     DECDEG       i       g    g-i\n",
      "0  10.078462  40.667978  18.543  18.514 -0.029\n",
      "1  10.071896  40.651283  18.481  18.578  0.097\n",
      "2  10.012758  40.566219  18.167  18.631  0.464\n",
      "3  10.064004  40.462811  18.353  18.637  0.284\n",
      "4  10.060642  40.622356  18.760  18.627 -0.133\n"
     ]
    }
   ],
   "source": [
    "m001p = pd.read_csv('Cleaned/cm233p.ascd', delimiter=' ')\n",
    "new_columns = {'Dec': 'DECDEG', 'RA': 'RADEG'}\n",
    "m001p = m001p.rename(columns=new_columns)\n",
    "col = m001p.pop('g')\n",
    "m001p.insert(3, 'g', col)\n",
    "# m001p.iloc[:, [2, 3]] = m001p.iloc[:, [3, 2]]\n",
    "m001p['g-i'] = m001p['g'] - m001p['i']\n",
    "# ra_deg = float(row['RAh']) * 15 + float(row['RAm']) * 0.25 + float(row['RAs']) * 0.00416667\n",
    "# dec_deg = float(row['DEd']) + (float(row['DEm'])/60) + (float(row['DEs'])/3600)\n",
    "catalog_coord = SkyCoord(ra=m001p['RADEG'], dec=m001p['DECDEG'], unit=(u.hourangle, u.deg))\n",
    "m001p['RADEG'] = catalog_coord.ra.degree\n",
    "m001p['DECDEG'] = catalog_coord.dec.degree\n",
    "print(m001p.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "# y_values_prob = np.array(rfc.predict_proba(m001p))[0]\n",
    "# y_values = np.array(rfc.predict(m001p))[0]\n",
    "# print((y_values.shape))\n",
    "# idx = y_values[:, 0].argsort()[::-1]\n",
    "# sorted_arr = y_values[idx]\n",
    "# print(sorted_arr)\n",
    "# print(y_values)\n",
    "# print(y_values[0][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "# probs = np.array(rfc.predict_proba(m001p))[:, 0]\n",
    "#\n",
    "# # Sort the probabilities in descending order\n",
    "# sorted_probs_idx = probs.argsort()[::-1]\n",
    "# sorted_probs = probs[sorted_probs_idx]\n",
    "#\n",
    "# print(sorted_probs)\n",
    "# # Get the corresponding rows from the original DataFrame\n",
    "# sorted_rows = m001p.iloc[sorted_probs_idx].values.flatten()\n",
    "#\n",
    "# # Combine the rows and probabilities into a new DataFrame\n",
    "# result = pd.concat([sorted_rows, pd.DataFrame(sorted_probs, columns=['Probability of class 0'])], axis=1)\n",
    "#\n",
    "# # Display the result\n",
    "# print(result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "#\n",
    "# # Assuming 'rfc' is your RandomForestClassifier instance and 'm001p' is your input data\n",
    "#\n",
    "# # Get the predicted probabilities for each class\n",
    "# predicted_probabilities = np.array(rfc.predict_proba(m001p))\n",
    "# print(predicted_probabilities[0])\n",
    "# # Extract the probabilities for class 0 (first column)\n",
    "# class_0_probabilities = predicted_probabilities[1, :, 1]\n",
    "#\n",
    "# # Get the indices of the sorted probabilities in descending order\n",
    "# sorted_indices = np.argsort(class_0_probabilities)[::-1]\n",
    "# print(sorted_indices)\n",
    "# # Sort the probabilities using the sorted indices\n",
    "# sorted_class_0_probabilities = class_0_probabilities[sorted_indices]\n",
    "#\n",
    "# # Print the ordered list of best matches for class 0\n",
    "# print(sorted_class_0_probabilities)\n",
    "# # Assuming 'm001p' is a 2D numpy array or a pandas DataFrame\n",
    "#\n",
    "# # Get the index of the best match for class 0\n",
    "# best_match_data = []\n",
    "# for indic in sorted_indices:\n",
    "#     best_match_index = indic\n",
    "#\n",
    "#     # Get the data corresponding to the best match\n",
    "#     best_match_data.append(m001p.iloc[best_match_index])\n",
    "#\n",
    "# # Print the best match data\n",
    "# print(best_match_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "test_size=0 should be either positive and smaller than the number of samples 1470 or a float in the (0, 1) range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[101], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m cv \u001B[38;5;241m=\u001B[39m RepeatedStratifiedKFold(n_splits\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, n_repeats\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      5\u001B[0m scoring \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mf1\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrecall\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprecision\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 6\u001B[0m X_train, X_test, _, _ \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstratify\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m#Train SRF\u001B[39;00m\n\u001B[0;32m      8\u001B[0m SRF\u001B[38;5;241m.\u001B[39mfit(X_train, y_train)\n",
      "File \u001B[1;32mD:\\PycharmProjects\\Masters\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2562\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[0;32m   2559\u001B[0m arrays \u001B[38;5;241m=\u001B[39m indexable(\u001B[38;5;241m*\u001B[39marrays)\n\u001B[0;32m   2561\u001B[0m n_samples \u001B[38;5;241m=\u001B[39m _num_samples(arrays[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m-> 2562\u001B[0m n_train, n_test \u001B[38;5;241m=\u001B[39m \u001B[43m_validate_shuffle_split\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2563\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefault_test_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.25\u001B[39;49m\n\u001B[0;32m   2564\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2566\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m shuffle \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[0;32m   2567\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stratify \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\PycharmProjects\\Masters\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2181\u001B[0m, in \u001B[0;36m_validate_shuffle_split\u001B[1;34m(n_samples, test_size, train_size, default_test_size)\u001B[0m\n\u001B[0;32m   2173\u001B[0m train_size_type \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(train_size)\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mkind\n\u001B[0;32m   2175\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   2176\u001B[0m     test_size_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mi\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2177\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (test_size \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m n_samples \u001B[38;5;129;01mor\u001B[39;00m test_size \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m   2178\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m test_size_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2179\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (test_size \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m test_size \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m   2180\u001B[0m ):\n\u001B[1;32m-> 2181\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2182\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_size=\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m should be either positive and smaller\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2183\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m than the number of samples \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m or a float in the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2184\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(0, 1) range\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(test_size, n_samples)\n\u001B[0;32m   2185\u001B[0m     )\n\u001B[0;32m   2187\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   2188\u001B[0m     train_size_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mi\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2189\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (train_size \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m n_samples \u001B[38;5;129;01mor\u001B[39;00m train_size \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m   2190\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m train_size_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2191\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (train_size \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m train_size \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m   2192\u001B[0m ):\n\u001B[0;32m   2193\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2194\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_size=\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m should be either positive and smaller\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2195\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m than the number of samples \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m or a float in the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2196\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(0, 1) range\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(train_size, n_samples)\n\u001B[0;32m   2197\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: test_size=0 should be either positive and smaller than the number of samples 1470 or a float in the (0, 1) range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "SRF = RandomForestClassifier(n_estimators=150, random_state=0)\n",
    "#Create Stratified K-fold cross validation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "scoring = ('f1', 'recall', 'precision')\n",
    "X_train, X_test, _, _ = train_test_split(X, y, test_size=0, stratify=y)\n",
    "#Train SRF\n",
    "SRF.fit(X_train, y_train)\n",
    "#SRF prediction result\n",
    "class_labels = SRF.predict(m001p)\n",
    "print(class_labels)\n",
    "# class_labels = rfc.predict(m001p)\n",
    "\n",
    "# get the indices of the elements that are predicted to be class 0\n",
    "class_0_indices = np.where(class_labels == 1)[0]\n",
    "class_0_indices = np.unique(class_0_indices)\n",
    "# get the data from `m001p` corresponding to the class 0 predictions\n",
    "class_0_data = m001p.iloc[class_0_indices, :]\n",
    "print(class_0_data.head())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "m001p_scaled = scaler.transform(m001p)\n",
    "# Assuming 'rfc' is your RandomForestClassifier instance and 'm001p' is your input data\n",
    "\n",
    "# Get the predicted probabilities for each class\n",
    "\n",
    "#Evaluate SRF model\n",
    "# scores = cross_validate(SRF, X, y, scoring=scoring, cv=cv)\n",
    "# #Get average evaluation metrics\n",
    "# print('Mean f1: %.3f' % mean(scores['test_f1']))\n",
    "# print('Mean recall: %.3f' % mean(scores['test_recall']))\n",
    "# print('Mean precision: %.3f' % mean(scores['test_precision']))\n",
    "\n",
    "#Randomly spilt dataset to test and train set\n",
    "\n",
    "\n",
    "svm = SVC(kernel='rbf', random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm.predict(X_test_scaled)\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "svm_class_labels = svm.predict(m001p_scaled)\n",
    "print(set(svm_class_labels))\n",
    "# get the indices of the elements that are predicted to be class 0\n",
    "svm_indices = np.where(svm_class_labels == 0)[0]\n",
    "svm_indices = np.unique(svm_indices)\n",
    "# get the data from `m001p` corresponding to the class 0 predictions\n",
    "svm_data = m001p.iloc[svm_indices, :]\n",
    "print(svm_data.head())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(svm_data))\n",
    "print(len(class_0_data))\n",
    "merged_data = pd.merge(svm_data, class_0_data, how='inner')\n",
    "\n",
    "# create a new dataframe that contains only the rows that match between `svm_data` and `class_0_data`\n",
    "matched_data = svm_data[svm_data.isin(merged_data.to_dict(orient='list')).all(1)]\n",
    "\n",
    "print(matched_data.head())\n",
    "print(len(matched_data))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## create DS9 regions for 'confirmed' GCs\n",
    "concat_thin_confirmed_gcs = pd.DataFrame(matched_data)\n",
    "radius = '10.0\"'\n",
    "with open('ds9_regions/m233_cut.reg', 'w') as f:\n",
    "    f.write('global color=green dashlist=8 3 width=1 font=\"helvetica 10 normal roman\" select=1 highlite=1 dash=0 fixed=0 edit=1 move=1 delete=1 include=1 source=1 \\n')\n",
    "    f.write('fk5 \\n')\n",
    "    for n in range(0,len(concat_thin_confirmed_gcs)):\n",
    "        ra = str(concat_thin_confirmed_gcs.RADEG[n:n+1].values[0])\n",
    "        dec = str(concat_thin_confirmed_gcs.DECDEG[n:n+1].values[0])\n",
    "        f.write('circle('+ra+','+dec+','+radius+') \\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
